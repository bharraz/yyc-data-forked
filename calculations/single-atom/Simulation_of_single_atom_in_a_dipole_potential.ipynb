{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "The experiemental effort of loading single Na atom is proven to be quite challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Compare to other atoms that people have successfully trapped in a tweezer (Cs, Rb), we have identified many possible reasons including (not necessarily independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Smaller exited state hyperfine structure\n",
    "* Not as efficient sub-Doppler cooling\n",
    "* No accessible magic wavelength for D2\n",
    "* Stochastic heating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "However, due to the characteristic of out system (Lamb-Dicke parameter $\\eta\\approx1$), it is very hard to analytically study these effects. On the other hand, since the system is very simple, a single atom with very few degrees of freedom, it is possible to do a precise numerical study of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "1. [Quantum jump method](#Quantum-jump-method)\n",
    "\n",
    "2. [Split operator method](#Split-operator-method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Quantum jump method](https://en.wikipedia.org/wiki/Quantum_jump_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The starting point of the simulation is the master equation\n",
    "\n",
    "$$ \\small{\\dot \\rho=\\frac1{i\\hbar}[H,\\rho]+\\sum_mC_m\\rho C_m^\\dagger-\\frac12\\sum_m\\left(C_m^\\dagger C_m\\rho +\\rho C_m^\\dagger C_m\\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "However, doing a deterministic calculation of the master equation requires the whole density matrix and many high dimensional matrix operations which is not very efficient. Instead, we use the [quantum jump method](https://en.wikipedia.org/wiki/Quantum_jump_method) (or Monte Carlo wave function (MCWF) method) to estimate the density matrix (and any measurable quantity derived from it) by evolving a wave function probabilistically.\n",
    "\n",
    "#### Rough scatch of the quantum jump method\n",
    "\n",
    "We divide the evolution into the coherent part described by the Hamiltonian $H$ and the incoherent part described by the jump operators $C_m$. In each time step of the simulation, we probabilistically choose either to evolve the wave function with the quantum jump or the Hamiltonian. The probability for doing each jump is determined by the expectation value of the jump operator $p_m=\\langle C_m^\\dagger C_m \\rangle$.\n",
    "\n",
    "* If we decide to jump, the wave function is turned into the projection of $C_m$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "jump:\n",
    "\n",
    "$$ |\\psi(t+\\delta t)\\rangle=C_m|\\psi(t)\\rangle $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* If we decide not to jump, the wave function is evolved with the original hamiltonian, plus a correction term due to the jump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "not jump:\n",
    "\n",
    "$$ H'=H-\\frac{i\\hbar}{2}\\sum_m C_m^\\dagger C_m $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Since none of these operations are unitary, the wave function needs to be normalized at each time step.\n",
    "\n",
    "Detail description of the method is available in thie master thesis http://www.oq.ulg.ac.be/master_thesis_rc.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Split operator method](https://en.wikipedia.org/wiki/Split-step_method)\n",
    "\n",
    "The evolution with the Hamiltonian is done with the [split operator method](https://en.wikipedia.org/wiki/Split-step_method).\n",
    "\n",
    "The Hamiltonian of the system can be expressed as\n",
    "\n",
    "$$ H = H_x + H_p $$\n",
    "  \n",
    "where $H_x$ and $H_p$ are only functions of $x$ and $p$ respectively. The time evolution of the system is described by $e^{iH\\delta t}$. This is hard to calculate since it is not diagonal in a good basis and the diagonalization is very time consuming.\n",
    "\n",
    "So instead of directly calculation the exponential of the full Hamiltonian, we calculate the transformation $e^{iH_x\\delta t}$ and $e^{iH_p\\delta t}$ separately on the wave function. This is easy to do in the $x$ and $p$ basis and the tranformation of basis can be done very efficiently with [fast Fourier transformation](https://en.wikipedia.org/wiki/Fast_Fourier_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance optimization\n",
    "\n",
    "1. [Task parallelism](#Task-parallelism)\n",
    "\n",
    "2. [Reduce memory allocation](#Reduce-memory-allocation)\n",
    "\n",
    "3. [Memory locality](#Memory-locality)\n",
    "    \n",
    "4. [SIMD (Single instruction multiple data) (a.k.a. Data parallelism)][1]\n",
    "\n",
    "\n",
    "[1]: #SIMD-(Single-instruction-multiple-data)-(a.k.a.-Data-parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Task parallelism](https://en.wikipedia.org/wiki/Task_parallelism)\n",
    "\n",
    "Since each time evolution in a Monte-Carlo simulation and each Monte-Carlo simulation in a multi-parameter scan are all independent, this is a embarrassingly parallelizable problem and can be done very efficiently with multiple processes (until the initialization cost kicks in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reduce memory allocation\n",
    "\n",
    "Memory allocations are expensive (to be improved). Reused buffers, use inplace operation, avoid boxing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Memory locality](https://en.wikipedia.org/wiki/Locality_of_reference)\n",
    "\n",
    "The CPU is several orders of magnitude faster than the memory. Multiple levels of CPU Cache are introduced to hide this latency but they are much smaller than the main memory.\n",
    "    \n",
    "* It's very important to make sure the operations has very good memory locality to fully utilize the cache.\n",
    "    \n",
    "    This usually means the inner loop should iterate over the continuous dimension of an array so it is important to know how array are stored in memory.\n",
    "        \n",
    "    For example, in languages using row-major format (`C`/`C++`, `Mathematica`, `Python`) the preferred way to interate over an multi dimentional array is,\n",
    "        \n",
    "    ```c++\n",
    "    \n",
    "    for (int i = 0;i < n1;i++) {\n",
    "        for (int j = 0;j < n2;j++) {\n",
    "            A[i][j] = B[i][j] + C[i][j];\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "        \n",
    "    in languages using column-major format (`MATLAB`, `R`, `Julia`), the preferred way is\n",
    "        \n",
    "    ```julia\n",
    "    \n",
    "    for i in 1:n1\n",
    "        for j in 1:n2\n",
    "            A[j, i] = B[j, i] + C[j, i]\n",
    "        end\n",
    "    end\n",
    "    ```\n",
    "        \n",
    "    If it is necessary to iterate over both dimensions simultaneously (e.g. for matrix multiplication) it is sometimes beneficial to divide the array in to blocks to improve memory locality. See also [Locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference#Matrix_multiplication).\n",
    "\n",
    "* Predictable memory access patterns also help CPU prefetching.\n",
    "\n",
    "* There are more to consider for multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [SIMD (Single instruction multiple data)](https://en.wikipedia.org/wiki/SIMD) (a.k.a. [Data parallelism](https://en.wikipedia.org/wiki/Data_parallelism))\n",
    "\n",
    "Modern processors introduce instructions that can process multiple data at a time to speed up CPU bound data processing. Most popular ones on the x86 processors are the [SSE family](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions) and the more recent [AVX family](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions). ARM processors also introduce [Advanced SIMD (NEON)](https://en.wikipedia.org/wiki/ARM_architecture#NEON) since ARMv6.\n",
    "\n",
    "It is not always necessary to write assembly manually in order to exploit this function. The compiler can do the transformation automatically in many cases (in julia this is done using the [`@simd` macro](http://julia.readthedocs.org/en/latest/manual/performance-tips/?highlight=simd#performance-annotations)). However, this automatic optimization has some limitation\n",
    "\n",
    "* No control flow\n",
    "\n",
    "    This means no branches and no function calls. Simple branches can be replaced with branchless [`ifelse` function call](http://julia.readthedocs.org/en/latest/stdlib/base/?highlight=ifelse#Base.ifelse).\n",
    "\n",
    "* Strided load\n",
    "\n",
    "    Most SIMD instruction set only support loading continious block of memory (`AVX2` and `NEON` might have support for strided/structural load). This is not a problem for scalar types if the iteration is along a continious direction. However, for composite type, e.g. complex number, if they are stored as an array of structures (AoS), a better way to store them is structure of arrays (SoA). To see this, a complex AoS is stored in memory as\n",
    "    \n",
    "    ```\n",
    "    AoS complex: [r1][i1][r2][i2][r2][i2][r2][i2]...\n",
    "    ```\n",
    "\n",
    "    while a complex SoA is stored as\n",
    "\n",
    "    ```\n",
    "    SoA complex:\n",
    "        real_ary: [r1][r2][r3][r4]...\n",
    "        imag_ary: [i1][i2][i3][i4]...\n",
    "    ```\n",
    "\n",
    "    Since the real and imaginary part of the complex number will likely be doing very different operations in each iteration while each of them are doing similar things between iteration, the vectorized version of this will operate on a vector of the real part and a vector of the imaginary part, i.e.\n",
    "\n",
    "    ```\n",
    "    real_vec: [r1][r2][r3][r4]\n",
    "    imag_vec: [i1][i2][i3][i4]\n",
    "    ```\n",
    "\n",
    "    As one can easily see, this is not the natural layout for AoS but is very natual for SoA.\n",
    "\n",
    "* Data dependency and [alias analysis](https://en.wikipedia.org/wiki/Alias_analysis)\n",
    "\n",
    "    Another limitation of the SIMD optimization is the there shouldn't be any data dependency between different loop iterations, e.g. the following loop cannot easily be vectorized,\n",
    "\n",
    "    ```julia\n",
    "    \n",
    "    for i in 1:(n - 1)\n",
    "        A[i + 1] = A[i] * 2\n",
    "    end\n",
    "    ```\n",
    "\n",
    "    Sometimes, there can be false positive data dependency since the compiler cannot proof that certain memory operations are independent. e.g. if `struct` is an arbitrary mutable type and `field` is one of its field, the compile may not vectorize the following loop because it cannot prove that the address of `A` and `struct` are independent (i.e. they don't alias).\n",
    "\n",
    "    ```julia\n",
    "    \n",
    "    for i in 1:n\n",
    "        A[i] *= struct.field\n",
    "    end\n",
    "    ```\n",
    "\n",
    "    This can usually be solved by manually hoisting the load out of the loop, i.e. the compiler should be able to vectorize the following loop.\n",
    "\n",
    "    ```julia\n",
    "    \n",
    "    v = struct.field\n",
    "    for i in 1:n\n",
    "        A[i] *= v\n",
    "    end\n",
    "    ```\n",
    "\n",
    "    If the alias analysis cannot prove that `struct` and `A` are independent, manual load hoisting can improve performance even if the loop cannot be vectorized since it reduce memory access."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia-0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
